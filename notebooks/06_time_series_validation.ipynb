{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271c3e29",
   "metadata": {},
   "source": [
    "# 06 — Time-series validation and robustness checks\n",
    "\n",
    "This notebook performs a walk-forward (TimeSeriesSplit) validation on the engineered feature set, compares baseline and benchmark models, and summarizes the stability of results (XLE, ICLN). Outputs include fold-level metrics and summary tables saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8797343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "project_root = Path(\"..\")\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "outputs_dir = project_root / \"outputs\"\n",
    "plots_dir = outputs_dir / \"plots\"\n",
    "results_dir = outputs_dir / \"results\"\n",
    "\n",
    "r2_timeseries_dir = plots_dir / \"r2_timeseries\"\n",
    "\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "r2_timeseries_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf64e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = data_dir / \"model_features_2018_2024.parquet\"\n",
    "assert features_path.exists(), f\"Missing file: {features_path}\"\n",
    "\n",
    "data = pd.read_parquet(features_path)\n",
    "\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"XLE_target\", \"ICLN_target\"]\n",
    "\n",
    "exclude_cols = set(target_cols)\n",
    "for c in [\"Date\", \"date\", \"timestamp\"]:\n",
    "    if c in data.columns:\n",
    "        exclude_cols.add(c)\n",
    "\n",
    "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "\n",
    "X = data[feature_cols].copy()\n",
    "y = data[target_cols].copy()\n",
    "\n",
    "# Basic sanity checks\n",
    "assert all(c in data.columns for c in target_cols), f\"Missing targets: {set(target_cols) - set(data.columns)}\"\n",
    "assert len(X) == len(y) == len(data)\n",
    "\n",
    "# If a date column exists, enforce chronological order\n",
    "date_col = next((c for c in [\"Date\", \"date\", \"timestamp\"] if c in data.columns), None)\n",
    "if date_col is not None:\n",
    "    assert data[date_col].is_monotonic_increasing, f\"{date_col} is not sorted increasing\"\n",
    "\n",
    "train_frac = 0.80\n",
    "train_size = int(len(data) * train_frac)\n",
    "\n",
    "X_train, y_train = X.iloc[:train_size].copy(), y.iloc[:train_size].copy()\n",
    "X_test,  y_test  = X.iloc[train_size:].copy(), y.iloc[train_size:].copy()\n",
    "\n",
    "print(\"Total observations:\", len(data))\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred) -> dict:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    return {\n",
    "        \"n\": int(len(y_true)),\n",
    "        \"RMSE\": float(np.sqrt(mean_squared_error(y_true, y_pred))),\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"R2\": float(r2_score(y_true, y_pred)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740679c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "test_size = len(X_train) // (n_splits + 1)  # stable fold sizes\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcab370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "results = []\n",
    "targets = target_cols\n",
    "\n",
    "for split_id, (train_idx, val_idx) in enumerate(tscv.split(X_train), start=1):\n",
    "    X_tr = X_train.iloc[train_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "\n",
    "    print(f\"Split {split_id}: train={len(train_idx)}, val={len(val_idx)}\")\n",
    "\n",
    "    for target in targets:\n",
    "        y_tr = y_train[target].iloc[train_idx]\n",
    "        y_val = y_train[target].iloc[val_idx]\n",
    "\n",
    "        # Naive benchmark: zero return\n",
    "        y_pred_naive = np.zeros(len(y_val))\n",
    "\n",
    "        lr_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\"model\", LinearRegression()),\n",
    "            ]\n",
    "        )\n",
    "        lr_pipeline.fit(X_tr, y_tr)\n",
    "        y_pred_lr = lr_pipeline.predict(X_val)\n",
    "\n",
    "        rf_model = RandomForestRegressor(\n",
    "            n_estimators=200,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        rf_model.fit(X_tr, y_tr)\n",
    "        y_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "        for model_name, y_pred in [\n",
    "            (\"naive\", y_pred_naive),\n",
    "            (\"linear_regression\", y_pred_lr),\n",
    "            (\"random_forest\", y_pred_rf),\n",
    "        ]:\n",
    "            metrics = compute_metrics(y_val, y_pred)\n",
    "            metrics[\"split\"] = split_id\n",
    "            metrics[\"target\"] = target\n",
    "            metrics[\"model\"] = model_name\n",
    "            results.append(metrics)\n",
    "\n",
    "cv_results = pd.DataFrame(results)[\n",
    "    [\"split\", \"target\", \"model\", \"n\", \"RMSE\", \"MAE\", \"R2\"]\n",
    "]\n",
    "\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_summary = (\n",
    "    cv_results\n",
    "    .groupby([\"target\", \"model\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_n=(\"n\", \"mean\"),\n",
    "        mean_RMSE=(\"RMSE\", \"mean\"),\n",
    "        std_RMSE=(\"RMSE\", \"std\"),\n",
    "        mean_MAE=(\"MAE\", \"mean\"),\n",
    "        std_MAE=(\"MAE\", \"std\"),\n",
    "        mean_R2=(\"R2\", \"mean\"),\n",
    "        std_R2=(\"R2\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "cv_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17eaebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_path = results_dir / \"cv_results_timeseries.csv\"\n",
    "cv_summary_path = results_dir / \"cv_summary_timeseries.csv\"\n",
    "\n",
    "cv_results.to_csv(cv_results_path, index=False)\n",
    "cv_summary.to_csv(cv_summary_path, index=False)\n",
    "\n",
    "cv_results_path, cv_summary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c509327",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in target_cols:\n",
    "    subset = cv_results[cv_results[\"target\"] == target].sort_values([\"model\", \"split\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    for model_name in subset[\"model\"].unique():\n",
    "        model_data = subset[subset[\"model\"] == model_name].sort_values(\"split\")\n",
    "        ax.plot(model_data[\"split\"], model_data[\"R2\"], marker=\"o\", label=model_name)\n",
    "\n",
    "    ax.set_title(f\"{target} — R2 across TimeSeriesSplit folds\")\n",
    "    ax.set_xlabel(\"Split\")\n",
    "    ax.set_ylabel(\"R2\")\n",
    "    ax.set_xticks(sorted(subset[\"split\"].unique()))\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = r2_timeseries_dir / f\"r2_timeseries_{target}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a72d1",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "We compare fold-level and average out-of-sample metrics (RMSE, MAE, R²) for the naive baseline, Linear Regression, and Random Forest on XLE and ICLN. The main focus is (i) whether any model consistently improves over the baseline, and (ii) how stable performance is across time splits. Results should be interpreted as predictive stability checks on the chosen feature set rather than evidence of structural predictability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
