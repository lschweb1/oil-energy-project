{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b11449b",
   "metadata": {},
   "source": [
    "## 05 â€“ Model evaluation and out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ec198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")\n",
    "\n",
    "project_root = Path(\"..\")\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "outputs_dir = project_root / \"outputs\"\n",
    "plots_dir = outputs_dir / \"plots\"\n",
    "results_dir = outputs_dir / \"results\"\n",
    "\n",
    "error_hist_dir = plots_dir / \"error_hist\"\n",
    "true_vs_pred_dir = plots_dir / \"true_vs_pred\"\n",
    "\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "error_hist_dir.mkdir(parents=True, exist_ok=True)\n",
    "true_vs_pred_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_parquet(results_dir / \"y_test_targets.parquet\")\n",
    "y_pred_naive = pd.read_parquet(results_dir / \"y_pred_naive.parquet\")\n",
    "y_pred_linreg = pd.read_parquet(results_dir / \"y_pred_linreg.parquet\")\n",
    "y_pred_rf = pd.read_parquet(results_dir / \"y_pred_rf.parquet\")\n",
    "\n",
    "preds = {\n",
    "    \"naive\": y_pred_naive,\n",
    "    \"linreg\": y_pred_linreg,\n",
    "    \"rf\": y_pred_rf,\n",
    "}\n",
    "\n",
    "for name, y_pred in preds.items():\n",
    "    y_pred_aligned = y_pred.reindex(y_test.index)\n",
    "    if y_pred_aligned.isna().any().any():\n",
    "        raise ValueError(f\"{name}: predictions do not align with y_test index (missing rows after reindex).\")\n",
    "    preds[name] = y_pred_aligned\n",
    "\n",
    "y_pred_naive = preds[\"naive\"]\n",
    "y_pred_linreg = preds[\"linreg\"]\n",
    "y_pred_rf = preds[\"rf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: pd.Series, y_pred: pd.Series) -> dict[str, float]:\n",
    "    return {\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1740796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = {\n",
    "    \"naive\": y_pred_naive,\n",
    "    \"linreg\": y_pred_linreg,\n",
    "    \"rf\": y_pred_rf,\n",
    "}\n",
    "\n",
    "for name, df_pred in model_predictions.items():\n",
    "    if list(df_pred.columns) != list(y_test.columns):\n",
    "        raise ValueError(f\"{name}: prediction columns do not match y_test columns.\")\n",
    "\n",
    "rows = []\n",
    "for target in y_test.columns:\n",
    "    y_true = y_test[target]\n",
    "    for model_name, df_pred in model_predictions.items():\n",
    "        m = compute_metrics(y_true, df_pred[target])\n",
    "        m[\"target\"] = target\n",
    "        m[\"model\"] = model_name\n",
    "        rows.append(m)\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)[[\"target\", \"model\", \"RMSE\", \"MAE\", \"R2\"]]\n",
    "metrics_df = metrics_df.sort_values([\"target\", \"model\"]).reset_index(drop=True)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_csv_path = results_dir / \"model_performance_metrics.csv\"\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "\n",
    "metrics_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in y_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    ax.plot(y_test.index, y_test[target], label=\"true\", linewidth=1)\n",
    "\n",
    "    for model_name, df_pred in model_predictions.items():\n",
    "        ax.plot(y_test.index, df_pred[target], label=model_name, linewidth=1)\n",
    "\n",
    "    ax.set_title(f\"{target}: true vs predicted (test)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Daily log return\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(true_vs_pred_dir / f\"true_vs_pred_{target}.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd24890",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in y_test.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "    for model_name, df_pred in model_predictions.items():\n",
    "        err = df_pred[target] - y_test[target]\n",
    "        ax.hist(err, bins=40, alpha=0.5, label=model_name)\n",
    "\n",
    "    ax.set_title(f\"{target}: prediction error (test)\")\n",
    "    ax.set_xlabel(\"Prediction error\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(error_hist_dir / f\"error_hist_{target}.png\", dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e3ee5",
   "metadata": {},
   "source": [
    "## Interpretation (based on out-of-sample metrics)\n",
    "\n",
    "Model ranking is based on test-set RMSE, reported in model_performance_metrics.csv\n",
    "\n",
    "Key limitations are methodological: daily data, a single feature set driven by WTI-derived predictors, and potential regime changes (e.g., COVID-19, 2022 energy shock)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
