{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db00c9c",
   "metadata": {},
   "source": [
    "# 04 — Modeling: train/test split, models, evaluation\n",
    "\n",
    "We build predictive models on the engineered dataset using a time-based train/test split. We report out-of-sample performance (RMSE, MAE, R²) for a naive baseline, linear regression, and a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6130d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = Path(\"../data/model_features_2018_2024.parquet\")\n",
    "assert features_path.exists()\n",
    "\n",
    "model_df = pd.read_parquet(features_path)\n",
    "model_df.index = pd.to_datetime(model_df.index)\n",
    "model_df = model_df.sort_index()\n",
    "\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e608c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"XLE_target\", \"ICLN_target\"]\n",
    "missing_targets = set(target_cols) - set(model_df.columns)\n",
    "assert not missing_targets, f\"Missing target columns: {missing_targets}\"\n",
    "\n",
    "feature_cols = [c for c in model_df.columns if c not in target_cols]\n",
    "\n",
    "X = model_df[feature_cols].copy()\n",
    "y = model_df[target_cols].copy()\n",
    "\n",
    "print(\"Number of features:\", X.shape[1])\n",
    "print(\"Target columns:\", target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(\"Train period:\", X_train.index.min(), \"to\", X_train.index.max())\n",
    "print(\"Test period:\", X_test.index.min(), \"to\", X_test.index.max())\n",
    "print(\"Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a78a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true: pd.DataFrame, y_pred: pd.DataFrame, model_name: str) -> dict:\n",
    "    targets = y_true.columns.tolist()\n",
    "\n",
    "    rmse = {t: np.sqrt(mean_squared_error(y_true[t], y_pred[t])) for t in targets}\n",
    "    mae = {t: mean_absolute_error(y_true[t], y_pred[t]) for t in targets}\n",
    "    r2 = {t: r2_score(y_true[t], y_pred[t]) for t in targets}\n",
    "\n",
    "    results = {\"model\": model_name}\n",
    "    results.update({f\"rmse_{t}\": rmse[t] for t in targets})\n",
    "    results[\"rmse_avg\"] = float(np.mean(list(rmse.values())))\n",
    "    results.update({f\"mae_{t}\": mae[t] for t in targets})\n",
    "    results[\"mae_avg\"] = float(np.mean(list(mae.values())))\n",
    "    results.update({f\"r2_{t}\": r2[t] for t in targets})\n",
    "    results[\"r2_avg\"] = float(np.mean(list(r2.values())))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27238b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_naive = pd.DataFrame(\n",
    "    0.0,\n",
    "    index=y_test.index,\n",
    "    columns=target_cols,\n",
    ")\n",
    "\n",
    "results_naive = evaluate_predictions(y_test, y_pred_naive, model_name=\"Naive: zero return\")\n",
    "display(pd.Series(results_naive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bbc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"regressor\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "linreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_linreg = pd.DataFrame(\n",
    "    linreg_pipeline.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=target_cols,\n",
    ")\n",
    "\n",
    "results_linreg = evaluate_predictions(y_test, y_pred_linreg, model_name=\"Linear Regression\")\n",
    "display(pd.Series(results_linreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02541183",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = pd.DataFrame(\n",
    "    rf_model.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=target_cols,\n",
    ")\n",
    "\n",
    "results_rf = evaluate_predictions(y_test, y_pred_rf, model_name=\"Random Forest\")\n",
    "display(pd.Series(results_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203460c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([results_naive, results_linreg, results_rf])\n",
    "\n",
    "metric_cols = sorted([c for c in results_df.columns if c != \"model\"])\n",
    "results_df = results_df[[\"model\"] + metric_cols]\n",
    "\n",
    "if \"rmse_avg\" in results_df.columns:\n",
    "    results_df = results_df.sort_values(\"rmse_avg\", ascending=True)\n",
    "\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "outputs_dir = Path(\"..\") / \"outputs\"\n",
    "results_dir = outputs_dir / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "y_test.to_parquet(results_dir / \"y_test_targets.parquet\")\n",
    "y_pred_naive.to_parquet(results_dir / \"y_pred_naive.parquet\")\n",
    "y_pred_linreg.to_parquet(results_dir / \"y_pred_linreg.parquet\")\n",
    "y_pred_rf.to_parquet(results_dir / \"y_pred_rf.parquet\")\n",
    "\n",
    "print(f\"Saved predictions to: {results_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
